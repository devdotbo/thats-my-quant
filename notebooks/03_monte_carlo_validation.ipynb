{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Validation Demonstration\n",
    "\n",
    "This notebook demonstrates how to use the Monte Carlo validation framework to assess the statistical robustness of trading strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.strategies.examples.moving_average import MovingAverageCrossover\n",
    "from src.backtesting.engines.vectorbt_engine import VectorBTEngine\n",
    "from src.validation.monte_carlo import (\n",
    "    MonteCarloValidator,\n",
    "    ResamplingMethod,\n",
    "    ConfidenceLevel\n",
    ")\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate Sample Data and Run Backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data (1 year of daily data)\n",
    "dates = pd.date_range('2023-01-01', '2024-01-01', freq='D')\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate realistic price data with trend\n",
    "trend = np.linspace(100, 120, len(dates))\n",
    "noise = np.random.normal(0, 2, len(dates)).cumsum()\n",
    "prices = trend + noise\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'open': prices + np.random.uniform(-0.5, 0.5, len(dates)),\n",
    "    'high': prices + np.random.uniform(0, 1, len(dates)),\n",
    "    'low': prices - np.random.uniform(0, 1, len(dates)),\n",
    "    'close': prices,\n",
    "    'volume': np.random.uniform(1000000, 5000000, len(dates))\n",
    "}, index=dates)\n",
    "\n",
    "# Ensure price consistency\n",
    "data['high'] = data[['open', 'close', 'high']].max(axis=1)\n",
    "data['low'] = data[['open', 'close', 'low']].min(axis=1)\n",
    "\n",
    "print(f\"Data shape: {data.shape}\")\n",
    "print(f\"Date range: {data.index[0]} to {data.index[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run backtest with Moving Average strategy\n",
    "strategy = MovingAverageCrossover(parameters={\n",
    "    'fast_period': 10,\n",
    "    'slow_period': 30,\n",
    "    'ma_type': 'sma'\n",
    "})\n",
    "\n",
    "engine = VectorBTEngine()\n",
    "backtest_result = engine.run_backtest(\n",
    "    strategy=strategy,\n",
    "    data=data,\n",
    "    initial_capital=100000,\n",
    "    commission=0.001\n",
    ")\n",
    "\n",
    "# Display original backtest metrics\n",
    "print(\"Original Backtest Metrics:\")\n",
    "for metric, value in backtest_result.metrics.items():\n",
    "    print(f\"{metric:20s}: {value:10.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Run Monte Carlo Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Monte Carlo validator\n",
    "mc_validator = MonteCarloValidator(\n",
    "    n_simulations=1000,\n",
    "    confidence_levels=[0.95, 0.99],\n",
    "    resampling_method=ResamplingMethod.BOOTSTRAP,\n",
    "    random_seed=42\n",
    ")\n",
    "\n",
    "# Run validation\n",
    "mc_result = mc_validator.run_validation(\n",
    "    backtest_result=backtest_result,\n",
    "    n_jobs=-1  # Use all CPUs\n",
    ")\n",
    "\n",
    "print(f\"\\nCompleted {len(mc_result.simulation_results)} Monte Carlo simulations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analyze Confidence Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display confidence intervals for key metrics\n",
    "metrics_to_show = ['total_return', 'sharpe_ratio', 'max_drawdown', 'win_rate']\n",
    "\n",
    "print(\"\\nConfidence Intervals:\")\n",
    "print(\"=\"*80)\n",
    "for metric in metrics_to_show:\n",
    "    if metric in mc_result.confidence_intervals:\n",
    "        print(f\"\\n{metric.upper()}:\")\n",
    "        print(f\"  Original: {backtest_result.metrics[metric]:10.4f}\")\n",
    "        \n",
    "        for cl in [0.95, 0.99]:\n",
    "            ci = mc_result.confidence_intervals[metric][cl]\n",
    "            print(f\"  {int(cl*100)}% CI: [{ci['lower']:10.4f}, {ci['upper']:10.4f}]\")\n",
    "            print(f\"  Mean: {ci['mean']:10.4f}, Median: {ci['median']:10.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize Metric Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distributions of key metrics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, metric in enumerate(metrics_to_show):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Extract metric values from simulations\n",
    "    values = [r[metric] for r in mc_result.simulation_results]\n",
    "    \n",
    "    # Plot histogram\n",
    "    ax.hist(values, bins=50, alpha=0.7, color='blue', edgecolor='black')\n",
    "    \n",
    "    # Add original value line\n",
    "    original_value = backtest_result.metrics[metric]\n",
    "    ax.axvline(original_value, color='red', linestyle='--', linewidth=2,\n",
    "               label=f'Original: {original_value:.3f}')\n",
    "    \n",
    "    # Add confidence interval lines\n",
    "    ci_95 = mc_result.confidence_intervals[metric][0.95]\n",
    "    ax.axvline(ci_95['lower'], color='green', linestyle=':', linewidth=1.5)\n",
    "    ax.axvline(ci_95['upper'], color='green', linestyle=':', linewidth=1.5,\n",
    "               label=f\"95% CI: [{ci_95['lower']:.3f}, {ci_95['upper']:.3f}]\")\n",
    "    \n",
    "    ax.set_title(f'Distribution of {metric.replace(\"_\", \" \").title()}')\n",
    "    ax.set_xlabel('Value')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.legend()\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Risk Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get risk metrics\n",
    "risk_metrics = mc_result.get_risk_metrics()\n",
    "\n",
    "print(\"\\nRisk Metrics:\")\n",
    "print(\"=\"*50)\n",
    "for metric, value in risk_metrics.items():\n",
    "    if 'probability' in metric or 'risk' in metric:\n",
    "        print(f\"{metric:35s}: {value:6.2%}\")\n",
    "    else:\n",
    "        print(f\"{metric:35s}: {value:10.4f}\")\n",
    "\n",
    "# Get percentile outcomes\n",
    "percentiles = mc_result.get_percentile_outcomes([5, 10, 25, 50, 75, 90, 95])\n",
    "\n",
    "print(\"\\nPercentile Analysis for Total Return:\")\n",
    "print(\"=\"*40)\n",
    "for p, value in percentiles['total_return'].items():\n",
    "    print(f\"{p:3.0f}th percentile: {value:8.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compare Different Resampling Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Monte Carlo with different resampling methods\n",
    "methods = [\n",
    "    ResamplingMethod.BOOTSTRAP,\n",
    "    ResamplingMethod.BLOCK,\n",
    "    ResamplingMethod.STATIONARY_BOOTSTRAP\n",
    "]\n",
    "\n",
    "method_results = {}\n",
    "for method in methods:\n",
    "    validator = MonteCarloValidator(\n",
    "        n_simulations=500,\n",
    "        confidence_levels=[0.95],\n",
    "        resampling_method=method,\n",
    "        random_seed=42\n",
    "    )\n",
    "    \n",
    "    method_results[method.value] = validator.run_validation(\n",
    "        backtest_result=backtest_result,\n",
    "        n_simulations=500\n",
    "    )\n",
    "\n",
    "# Compare results\n",
    "print(\"\\nComparison of Resampling Methods:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Method':20s} {'Sharpe Mean':>12s} {'Sharpe Std':>12s} {'95% CI Width':>15s}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for method_name, result in method_results.items():\n",
    "    ci = result.confidence_intervals['sharpe_ratio'][0.95]\n",
    "    ci_width = ci['upper'] - ci['lower']\n",
    "    print(f\"{method_name:20s} {ci['mean']:12.4f} {ci['std']:12.4f} {ci_width:15.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results for further analysis\n",
    "import os\n",
    "\n",
    "# Create results directory if it doesn't exist\n",
    "os.makedirs('../results/monte_carlo', exist_ok=True)\n",
    "\n",
    "# Export to CSV\n",
    "mc_result.export_metrics_to_csv('../results/monte_carlo/mc_simulation_results.csv')\n",
    "print(\"Exported simulation results to CSV\")\n",
    "\n",
    "# Export summary to JSON\n",
    "mc_result.export_summary_to_json('../results/monte_carlo/mc_summary.json')\n",
    "print(\"Exported summary statistics to JSON\")\n",
    "\n",
    "# Display summary\n",
    "summary = mc_result.get_summary()\n",
    "print(f\"\\nValidation Summary:\")\n",
    "print(f\"- Simulations: {summary['n_simulations']}\")\n",
    "print(f\"- Original Sharpe: {summary['original_metrics']['sharpe_ratio']:.4f}\")\n",
    "print(f\"- 95% CI Sharpe: [{summary['confidence_intervals']['sharpe_ratio'][0.95]['lower']:.4f}, \"\n",
    "      f\"{summary['confidence_intervals']['sharpe_ratio'][0.95]['upper']:.4f}]\")\n",
    "\n",
    "if 'risk_metrics' in summary:\n",
    "    print(f\"- Probability of negative Sharpe: {summary['risk_metrics']['sharpe_below_zero_probability']:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}